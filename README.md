# Fine-Tunning-Gemma-with-QLoRA-4bit-and-8bit
This repository contains two Jupyter notebooks for fine-tuning the Gemma model using QLoRA with 4-bit and 8-bit quantization. QLoRA leverages techniques such as 4-bit NormalFloat (NF4) and 8-bit quantization to optimize model storage and computation without sacrificing much accuracy. The comparison highlights the differences in memory usages.
